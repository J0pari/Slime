# Training configuration for Slime Mold Transformer

training:
  num_epochs: 100
  batch_size: 32
  learning_rate: 0.001
  device: cuda  # cuda or cpu
  gradient_clip_norm: 1.0
  log_interval: 10
  eval_interval: 100
  checkpoint_interval: 1000

  # Phased training (Invariant #9)
  warmup_steps: 1000    # Phase 1: No lifecycle changes
  gentle_steps: 5000    # Phase 2: Births only
  # After gentle_steps: Full dynamics (births + deaths + culling)

optimizer:
  type: adam  # adam, adamw, sgd
  lr: 0.001
  betas: [0.9, 0.999]
  eps: 1.0e-08
  weight_decay: 0.0001

scheduler:
  type: cosine  # cosine, linear, constant
  warmup_steps: 1000
  min_lr: 1.0e-06

loss:
  weights:
    reconstruction: 1.0
    rank_regularization: 0.1
    coherence_regularization: 0.1
    diversity: 0.05
    archive_coverage: 0.05
    fitness_variance: 0.02

# Lifecycle management (Invariant #9)
lifecycle:
  max_pool_size: 64
  max_archive_size: 1000
  max_loss_ratio: 10.0
  loss_ema_alpha: 0.1
  check_interval: 100
  freeze_on_divergence: true

# Pool configuration
pool:
  initial_size: 16
  max_size: 64
  spawn_threshold: 0.7      # Fitness threshold for spawning
  cull_threshold: 0.3       # Fitness threshold for culling
  spawn_batch_size: 4
  cull_batch_size: 2

# Archive configuration (MAP-Elites)
archive:
  grid_size: [50, 50]       # Behavioral space discretization (rank, coherence)
  dimensions: 2             # Rank and coherence
  max_size: 1000
  elite_threshold: 0.5      # Fitness threshold for archive insertion

# Timescale separation (Invariant #6)
timescales:
  fast: 1       # Weight updates, fitness tracking (every step)
  medium: 100   # Archive updates, spawn decisions (every 100 steps)
  slow: 1000    # Culling, memory budget (every 1000 steps)

# Model architecture
model:
  sensory_dim: 128
  latent_dim: 256
  head_dim: 64
  initial_pseudopods: 16
  use_triton: true  # Use Triton GPU kernels if available

# Data
data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  num_workers: 4
  prefetch_factor: 2
  pin_memory: true

# Checkpointing
checkpoint:
  save_dir: checkpoints
  save_interval: 1000
  keep_last_n: 5
  save_optimizer: true
  save_scheduler: true

# Logging
logging:
  log_dir: logs
  tensorboard: true
  wandb: false
  wandb_project: slime-mold-transformer
  log_gradients: false
  log_weights: false

# Reproducibility
seed: 42
deterministic: false  # Set to true for fully deterministic training (slower)
